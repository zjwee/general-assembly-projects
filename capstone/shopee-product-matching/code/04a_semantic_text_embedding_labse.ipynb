{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_semantic_text_embedding_labse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjX4J7mJvbtR"
      },
      "source": [
        "## General"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-RW59SOcfI"
      },
      "source": [
        "This colab notebook generates semantic embeddings on the product titles using Google Language-agnostic BERT Sentence Embedding ([LaBSE](https://tfhub.dev/google/LaBSE/1)).\n",
        "\n",
        "LaBSE is a multilingual sentence semantic encoder that takes in sentences as input and generates a feature embedding of size 768."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPBTrvDduj6r",
        "outputId": "0981ed8f-e820-456f-d2c9-887af9a966ee"
      },
      "source": [
        "# Confirm GPU is running\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr 23 12:40:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWooiTBxuj4F"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZGfx1k7uj1L",
        "outputId": "681e7b9b-1254-4b9a-ad5a-9264676db2d3"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 28.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=36c7f86f77966c43ead6dd3f99709848932f0f11951c717a490ef6860fc30eaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=c0fbbf48694c70f068766593edba4d425046492163246e8d2946d1909b7c619f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=e672bbd75a47dda93fffd9ba30d9c546cb5cf9e1eb33553b8c7da783a17ee214\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roe-VDU-ujyb"
      },
      "source": [
        "# Load train data\n",
        "train = pd.DataFrame(np.load('/content/drive/MyDrive/General Assembly - Data Science Immersive/shopee-product-matching/datasets/train.npy', allow_pickle=True),\n",
        "                     columns=['posting_id', 'image', 'image_phash', 'title', 'label_group', 'matches', 'image_duplicates'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQRKFKovejS"
      },
      "source": [
        "## Generate Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxvzlClQwBO4",
        "outputId": "ed31c187-1fa9-4b71-de99-a76c29b85e6c"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tzaYHo7ujwL"
      },
      "source": [
        "# Potential stop words:\n",
        "shopee_words = [# Sales words:\n",
        "                'free', 'gift', 'give', 'get', 'ready', 'stock', 'stocks', 'stok',\n",
        "                'ori', 'original', 'official', 'new', 'latest',\n",
        "                'import', 'low', 'price', 'cheap', 'vip', 'discount', 'warranty',\n",
        "                'promo', 'promotion', 'buy', 'buyer', 'shop', 'shopper', 'shopping',\n",
        "                'bigsale', 'sale', 'sell', 'seller', 'resell', 'reseller',\n",
        "                'all', 'any', 'full', 'include', 'includes', 'inclusive', 'tax',\n",
        "    \n",
        "                # Units\n",
        "                'pieces', 'piece', 'pcs', 'pc', 'box', 'boxes', 'pack', 'packs', 'packet', 'packets', 'paket', 'package',\n",
        "                'set', 'sets', 'size', 'roll', 'rolls', 'sachet', 'sachets'\n",
        "                \n",
        "                # Dimensions\n",
        "                'ml', 'l', 'litre', 'liter', 'g', 'gr', 'gram', 'kg', 'kilo', 'kilogram',\n",
        "                'mm', 'cm', 'm', 'meter', 'metre', 'yard', 'inch', 'x',\n",
        "    \n",
        "                # Miscellaneous alphabets\n",
        "                'c', 'xe', 'f', 'b', 'v', 'xa',\n",
        "                \n",
        "                # Location words:\n",
        "                'shopee', 'indonesia', 'indonesian', 'indo', 'id', 'jakarta', 'local', 'lokal',\n",
        "    \n",
        "                # English descriptors:\n",
        "                'fashion', 'colour', 'color', 'design',\n",
        "                'plus', 'pro', 'mini', 'premium', 'pro', 'super', 'extra', 'big', 'small',\n",
        "                \n",
        "                # Indonesian descriptors:\n",
        "                'bpom', 'muat', 'cod', 'murah', 'isi', 'warna', 'pajak', 'garansi', 'beli', 'gratis',\n",
        "                'terbaru', 'harga', 'resmi',\n",
        "                \n",
        "]\n",
        "\n",
        "# Add NLTK English and Indonesian stop words\n",
        "stop_words = stopwords.words('english') + \\\n",
        "             stopwords.words('indonesian') + \\\n",
        "             shopee_words\n",
        "\n",
        "stop_words = list(set(stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F6VrVRFwILc"
      },
      "source": [
        "# Create function for generating tokens from titles\n",
        "def process_tokens(title, stop_words, tokenizer):\n",
        "    words = tokenizer.tokenize(title.lower())\n",
        "    return ' '.join([word for word in words if word not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYsFY_J-ujrd"
      },
      "source": [
        "# Create same token sets 2 and 3 as in notebook 04_text_embeddings\n",
        "tokenizer_1 = nltk.tokenize.RegexpTokenizer('[a-zA-Z0-9]+')\n",
        "tokens_2 = train['title'].map(lambda x: process_tokens(x, stop_words, tokenizer_1)).to_numpy()\n",
        "tokens_3 = train['title'].map(lambda x: process_tokens(x, [], tokenizer_1)).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxrydt1vjX4"
      },
      "source": [
        "## Load LaBSE Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAms9ZzEPEN4"
      },
      "source": [
        "This section follows the Google LaBSE API to generate sentence embeddings from input strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9GEMSlQujkE"
      },
      "source": [
        "def get_model(model_url, max_seq_length):\n",
        "    labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "\n",
        "    # Define input.\n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                             name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                         name=\"input_mask\")\n",
        "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                          name=\"segment_ids\")\n",
        "\n",
        "    # LaBSE layer.\n",
        "    pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "    # The embedding is l2 normalized.\n",
        "    pooled_output = tf.keras.layers.Lambda(\n",
        "          lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
        "\n",
        "    # Define model.\n",
        "    return tf.keras.Model(\n",
        "            inputs=[input_word_ids, input_mask, segment_ids],\n",
        "            outputs=pooled_output), labse_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyHoFx7zvpR8"
      },
      "source": [
        "# Set max sequence length\n",
        "max_seq_length = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDfjGgeuYIp"
      },
      "source": [
        "labse_model, labse_layer = get_model(\n",
        "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhSRodjtfcX"
      },
      "source": [
        "# Examine BERT tokenizer\n",
        "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUIWno7Zts1F",
        "outputId": "ecc5baaa-15df-47aa-9537-4929526b044a"
      },
      "source": [
        "# Check some tokenized titles\n",
        "tokens_2[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['paper bag victoria secret',\n",
              "       'double tape 3m vhb 12 4 5 double foam tape',\n",
              "       'maling tts canned pork luncheon meat 397',\n",
              "       'daster batik lengan pendek motif acak campur leher kancing dpt001 00 batik karakter alhadi',\n",
              "       'nescafe xc3 x89clair latte 220ml',\n",
              "       'celana wanita bb 45 84 harem wanita', 'jubah anak 1 12 thn',\n",
              "       'kulot plisket salur candy plisket wish kulot kulot pelangi hieka kulot',\n",
              "       'logu tempelan kulkas magnet angka tempelan angka magnet',\n",
              "       'sepatu pantofel kulit keren kerja kantor laki pria cowok dinas formal pesta kickers'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4tZmGsTtswG",
        "outputId": "c9c9ca68-4c02-4154-e327-cec4802f62bd"
      },
      "source": [
        "# Check how our tokenized titles get re-tokenized by BERT\n",
        "for input_string in tokens_2[:10]:\n",
        "  print(tokenizer.tokenize(input_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['paper', 'bag', 'victoria', 'secret']\n",
            "['double', 'tape', '3m', 'v', '##hb', '12', '4', '5', 'double', 'foam', 'tape']\n",
            "['maling', 'tts', 'canne', '##d', 'pork', 'lunch', '##eon', 'meat', '397']\n",
            "['das', '##ter', 'batik', 'lengan', 'pendek', 'motif', 'acak', 'campur', 'leher', 'kan', '##cing', 'dpt', '##001', '00', 'batik', 'karakter', 'al', '##hadi']\n",
            "['nes', '##cafe', 'x', '##c', '##3', 'x', '##89', '##clair', 'latte', '220', '##ml']\n",
            "['celana', 'wanita', 'bb', '45', '84', 'harem', 'wanita']\n",
            "['jubah', 'anak', '1', '12', 'thn']\n",
            "['kulo', '##t', 'plis', '##ket', 'salu', '##r', 'candy', 'plis', '##ket', 'wish', 'kulo', '##t', 'kulo', '##t', 'pelan', '##gi', 'hie', '##ka', 'kulo', '##t']\n",
            "['logu', 'tempel', '##an', 'kulkas', 'magnet', 'angka', 'tempel', '##an', 'angka', 'magnet']\n",
            "['sepatu', 'panto', '##fel', 'kulit', 'keren', 'kerja', 'kantor', 'laki', 'pria', 'cowok', 'dinas', 'formal', 'pesta', 'kick', '##ers']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqpxNuMEvp8Z"
      },
      "source": [
        "def create_input(input_strings, tokenizer, max_seq_length):\n",
        "    \n",
        "    input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
        "    for input_string in input_strings:\n",
        "        \n",
        "        # Tokenize input.\n",
        "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "        sequence_length = min(len(input_ids), max_seq_length)\n",
        "\n",
        "        # Padding or truncation.\n",
        "        if len(input_ids) >= max_seq_length:\n",
        "            input_ids = input_ids[:max_seq_length]\n",
        "        else:\n",
        "            input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "        input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "        input_ids_all.append(input_ids)\n",
        "        input_mask_all.append(input_mask)\n",
        "        segment_ids_all.append([0] * max_seq_length)\n",
        "\n",
        "    return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eph14Tn6vtQI"
      },
      "source": [
        "def encode(input_text):\n",
        "    input_ids, input_mask, segment_ids = create_input(input_text, tokenizer, max_seq_length)\n",
        "    return labse_model([input_ids, input_mask, segment_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0NS4lMNvtsw"
      },
      "source": [
        "## Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqVkGDDRvvVo"
      },
      "source": [
        "# As the dataset is large, we will run the embedding in chunks\n",
        "chunk_size = 3000\n",
        "chunks = np.arange(np.ceil(len(train) / chunk_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kIqryDUvvTg",
        "outputId": "cbdb6459-aa9e-46b9-9748-2e7b80accf78"
      },
      "source": [
        "# Generate text embeddings from LaBSE model in chunks for tokens set 2\n",
        "# Initialize embeddings list\n",
        "embeddings = []\n",
        "\n",
        "# Iterate through chunks\n",
        "for i in chunks:\n",
        "    # Start and end index\n",
        "    start = int(i * chunk_size)\n",
        "    end = int((i + 1) * chunk_size)\n",
        "\n",
        "    # Get tokens\n",
        "    tokens = tokens_2[start:end]\n",
        "\n",
        "    # Generate embeddings\n",
        "    text_embeddings = encode(tokens)\n",
        "\n",
        "    # Append to embeddings list\n",
        "    embeddings.append(text_embeddings)\n",
        "\n",
        "    # Print status\n",
        "    print(f'Chunk {i} completed')\n",
        "\n",
        "text_labse_embeddings_2 = np.concatenate(embeddings)\n",
        "\n",
        "# Delete temporary variables to free memory\n",
        "del embeddings\n",
        "del tokens\n",
        "del text_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chunk 0.0 completed\n",
            "Chunk 1.0 completed\n",
            "Chunk 2.0 completed\n",
            "Chunk 3.0 completed\n",
            "Chunk 4.0 completed\n",
            "Chunk 5.0 completed\n",
            "Chunk 6.0 completed\n",
            "Chunk 7.0 completed\n",
            "Chunk 8.0 completed\n",
            "Chunk 9.0 completed\n",
            "Chunk 10.0 completed\n",
            "Chunk 11.0 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AExN2BsIvvQP",
        "outputId": "ea45404a-f0e0-4e3c-c74a-7387aa9c3790"
      },
      "source": [
        "text_labse_embeddings_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34250, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCETVIvvvOU"
      },
      "source": [
        "# Save embeddings as npy file\n",
        "np.save('/content/drive/MyDrive/General Assembly - Data Science Immersive/shopee-product-matching/datasets/text_labse_embeddings_2.npy', text_labse_embeddings_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJvnYfzyvvL1",
        "outputId": "185a6537-0e76-4eec-c9d7-b6ac9d6bb72a"
      },
      "source": [
        "# Generate text embeddings from LaBSE model in chunks for tokens set 3\n",
        "# Initialize embeddings list\n",
        "embeddings = []\n",
        "\n",
        "# Iterate through chunks\n",
        "for i in chunks:\n",
        "    # Start and end index\n",
        "    start = int(i * chunk_size)\n",
        "    end = int((i + 1) * chunk_size)\n",
        "\n",
        "    # Get tokens\n",
        "    tokens = tokens_3[start:end]\n",
        "\n",
        "    # Generate embeddings\n",
        "    text_embeddings = encode(tokens)\n",
        "\n",
        "    # Append to embeddings list\n",
        "    embeddings.append(text_embeddings)\n",
        "\n",
        "    # Print status\n",
        "    print(f'Chunk {i} completed')\n",
        "\n",
        "text_labse_embeddings_3 = np.concatenate(embeddings)\n",
        "\n",
        "# Delete temporary variables to free memory\n",
        "del embeddings\n",
        "del tokens\n",
        "del text_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chunk 0.0 completed\n",
            "Chunk 1.0 completed\n",
            "Chunk 2.0 completed\n",
            "Chunk 3.0 completed\n",
            "Chunk 4.0 completed\n",
            "Chunk 5.0 completed\n",
            "Chunk 6.0 completed\n",
            "Chunk 7.0 completed\n",
            "Chunk 8.0 completed\n",
            "Chunk 9.0 completed\n",
            "Chunk 10.0 completed\n",
            "Chunk 11.0 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBJjAxKRvvJp",
        "outputId": "e1c2d1c6-095a-497a-b70d-4b977f9e4553"
      },
      "source": [
        "text_labse_embeddings_3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34250, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlVl9mbovvHc"
      },
      "source": [
        "# Save embeddings as npy file\n",
        "np.save('/content/drive/MyDrive/General Assembly - Data Science Immersive/shopee-product-matching/datasets/text_labse_embeddings_3.npy', text_labse_embeddings_3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}