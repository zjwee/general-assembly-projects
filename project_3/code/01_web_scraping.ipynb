{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario:**\n",
    "\n",
    "[r/TalesFromTheCustomer](https://www.reddit.com/r/TalesFromTheCustomer/) and [r/TalesFromRetail](https://www.reddit.com/r/TalesFromRetail/) are similar subreddits describing customer service experiences, one from the point of view of the customer, and one from the service staff. \n",
    "\n",
    "These subreddits contain narratives of people's experiences and are thus rich in text data and suitable for natural language processing (NLP) modelling using a bag of words approach. As they are not too dissimilar in content, it is interesting to examine if a NLP machine learning model can distinguish between these 2 subreddits and in the process, identify the most frequent words associated with customer / retail experiences.\n",
    "\n",
    "Such a NLP model could be useful to retail executives who wish to understand more about the experiences of their customers and frontline staff. As such, the NLP model should preferably be interpretable, allowing inference on the significance of each word to the model's classification criteria.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "This project will scrape the Reddit API of these 2 subreddits to collect text data (title + main post). After data cleaning, exploratory data analysis will be performed to identify the most frequent words in each subreddit. \n",
    "\n",
    "The data will be split into a train and test set. Classification models will be fit on the train set and scored on the test set to identify the best performing model. F1 score will be used as the main metric. The best models will be analyzed to examine how their underlying algorithms could have affected their predictive ability on this bag of words classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "927 rows of data for r/TalesFromTheCustomer and 320 rows for r/TalesFromRetail were scraped and combined into a single dataframe. The columns 'title' and 'selftext' were combined into a single 'text' column. Lemmatized and stemmed versions of the text were created as new columns.\n",
    "\n",
    "A first level of screening was performed across each shortener (lemmatized, stemmed, raw unshortened text), each vectorizer (Count Vectorizer, TF-IDF Vectorizer), and each model (Logistic Regression, Multinomial Naive Bayes, Support Vector Machine, K-Nearest Neighbours, Extra Trees and Random Forest). \n",
    "\n",
    "Logistic Regression, Multinomial Naive Bayes and Support Vector Machine (each with Count Vectorizer) and K-Nearest Neighbours (with TF-IDF Vectorizer) were found to be the best performing models and grid searched on.\n",
    "\n",
    "After grid searching, Logistic Regression (test f1 score = 0.70) and Multinomial Naive Bayes (test f1 score = 0.72) were selected as the final models. If we want to **maximize precision and minimize false positives**, **Logistic Regression** would be the best model. If we want to **maximize recall and minimize false negatives**, **MultinomialNB** would be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract urls from r/TalesFromTheCustomer and r/TalesFromRetail\n",
    "cust_url = 'https://www.reddit.com/r/TalesFromTheCustomer.json'\n",
    "cust_req = requests.get(cust_url, headers={'User-agent': 'random_123'})\n",
    "\n",
    "ret_url = 'https://www.reddit.com/r/TalesFromRetail.json'\n",
    "ret_req = requests.get(ret_url, headers={'User-agent': 'random_123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status codes\n",
    "cust_req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/TalesFromTheCustomer.json\n",
      "19\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_luyo4v\n",
      "27\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_lmveps\n",
      "22\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_ldokfx\n",
      "4\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_l74h1j\n",
      "60\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_kzxa1m\n",
      "29\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_kx0mly\n",
      "56\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_kqosh6\n",
      "18\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_kj45dt\n",
      "52\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_kehnme\n",
      "44\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_k93rif\n",
      "42\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_k44zfg\n",
      "44\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_jz8yr1\n",
      "10\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_jrr5sx\n",
      "7\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_jkh2gl\n",
      "43\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_jg3eo8\n",
      "51\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_jbmzo9\n",
      "56\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_j6pvyy\n",
      "31\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_ixkqy9\n",
      "28\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_irkig7\n",
      "18\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_inz2r9\n",
      "37\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_iij89n\n",
      "39\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_iew2uf\n",
      "50\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_icj0eh\n",
      "50\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_i87n8i\n",
      "5\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_i38bt6\n",
      "11\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hztl4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weezi\\anaconda3\\envs\\dsi\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hv9p8i\n",
      "7\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_htii5x\n",
      "36\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hq5lsu\n",
      "3\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_ho32ng\n",
      "16\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hk73nx\n",
      "44\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hgi1fa\n",
      "3\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_hcaha0\n",
      "26\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_h8h2ki\n",
      "53\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_gvcf8s\n",
      "35\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_gpssyw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weezi\\anaconda3\\envs\\dsi\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (28,73,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_gkmuuj\n",
      "46\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json\n",
      "38\n",
      "https://www.reddit.com/r/TalesFromTheCustomer.json?after=t3_luyo4v\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# Perform 40 iterations of scraping r/TalesFromTheCustomer\n",
    "url = cust_url\n",
    "posts = []\n",
    "after = None\n",
    "\n",
    "# Set number of loops = 40 and generate random user-agents for each iteration\n",
    "num_loops = 40\n",
    "user_agents = [''.join(random.choice(string.hexdigits) for i in range(8)) for j in range(num_loops)]\n",
    "\n",
    "for i in range(num_loops):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after\n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers={'User-agent': user_agents[i]})\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "    \n",
    "    # Save updated csv file for each iteration\n",
    "    if i > 0:\n",
    "        prev_posts = pd.read_csv('customer.csv')\n",
    "        current_df = pd.DataFrame(posts)\n",
    "        new_df = pd.concat([prev_posts, current_df])\n",
    "        new_df.to_csv('customer.csv', index = False)\n",
    "        \n",
    "    else:\n",
    "        pd.DataFrame(posts).to_csv('customer.csv', index = False)\n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(2,60)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20470, 107)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final csv file\n",
    "cust_df = pd.read_csv('customer.csv')\n",
    "cust_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(927, 107)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates by 'selftext' column\n",
    "cust_df.drop_duplicates(subset='selftext',inplace=True)\n",
    "cust_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final csv file\n",
    "cust_df.to_csv('datasets/customer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/TalesFromRetail.json\n",
      "46\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lrpknf\n",
      "33\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lgsxw2\n",
      "40\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l815gs\n",
      "89\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l0zo17\n",
      "39\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kri44z\n",
      "67\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kj9kc1\n",
      "54\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kckgnu\n",
      "65\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k5kpqi\n",
      "62\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k17hda\n",
      "54\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jv37q0\n",
      "33\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jqz8tj\n",
      "59\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jkbyxd\n",
      "60\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jfmfhs\n",
      "85\n",
      "https://www.reddit.com/r/TalesFromRetail.json\n",
      "79\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lrpknf\n",
      "39\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lgsxw2\n",
      "42\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l815gs\n",
      "67\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l0zo17\n",
      "30\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kri44z\n",
      "82\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kj9kc1\n",
      "81\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kckgnu\n",
      "57\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k5kpqi\n",
      "36\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k17hda\n",
      "43\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jv37q0\n",
      "65\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jqz8tj\n",
      "74\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jkbyxd\n",
      "81\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jfmfhs\n",
      "41\n",
      "https://www.reddit.com/r/TalesFromRetail.json\n",
      "86\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lrpknf\n",
      "77\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_lgsxw2\n",
      "69\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l815gs\n",
      "43\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_l0zo17\n",
      "34\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kri44z\n",
      "64\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kj9kc1\n",
      "45\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_kckgnu\n",
      "42\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k5kpqi\n",
      "87\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_k17hda\n",
      "35\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jv37q0\n",
      "68\n",
      "https://www.reddit.com/r/TalesFromRetail.json?after=t3_jqz8tj\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# Perform 40 iterations of scraping r/TalesFromRetail\n",
    "url = ret_url\n",
    "posts = []\n",
    "after = None\n",
    "\n",
    "# Set number of loops = 40 and generate random user-agents for each iteration\n",
    "num_loops = 40\n",
    "user_agents = [''.join(random.choice(string.ascii_letters) for i in range(10)) for j in range(num_loops)]\n",
    "\n",
    "for i in range(num_loops):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after\n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers={'User-agent': user_agents[i]})\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "    \n",
    "    # Save updated csv file for each iteration\n",
    "    if i > 0:\n",
    "        prev_posts = pd.read_csv('retail.csv')\n",
    "        current_df = pd.DataFrame(posts)\n",
    "        new_df = pd.concat([prev_posts, current_df])\n",
    "        new_df.to_csv('retail.csv', index = False)\n",
    "        \n",
    "    else:\n",
    "        pd.DataFrame(posts).to_csv('retail.csv', index = False)\n",
    "    \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "    sleep_duration = random.randint(30,90)\n",
    "    print(sleep_duration)\n",
    "    time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19898, 104)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check final csv file\n",
    "ret_df = pd.read_csv('retail.csv')\n",
    "ret_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 104)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates by 'selftext' column\n",
    "ret_df.drop_duplicates(subset='selftext',inplace=True)\n",
    "ret_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final csv file\n",
    "ret_df.to_csv('datasets/retail.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw customer.csv and retail.csv have been deleted from main directory due to large file size.\n",
    "\n",
    "927 rows of data remain for r/TalesFromTheCustomer and 320 rows remain for r/TalesFromRetail after removing duplicates.\n",
    "\n",
    "In next section 02_modelling, the datasets will be processed and analysed. Classification models will be created and scored based on the processed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
